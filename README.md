# Sound Realty Pricing API â€“ phData Challenge

This project implements a production-style Machine Learning API for real estate price prediction, built for the phData take-home challenge.

---

## ğŸš€ Features
- Gradient Boosting Model (GBM) trained on housing + demographics data
- MLflow tracking of experiments and artifacts
- FastAPI app containerized with Docker + Gunicorn
- Batch and JSON (on-demand) prediction endpoints
- Auto-generated API docs via FastAPI (/docs and /redoc)
- CI pipeline with GitHub Actions

---

## ğŸ—ï¸ Setup

### 1. Clone & enter repo
```bat
git clone https://github.com/samehere1/demo.git
cd demo

2. Start services

docker compose up --build

This starts:

    API at http://localhost:8080

MLflow UI at http://127.0.0.1:5001

Check health:

curl http://localhost:8080/healthz

ğŸ“– API Docs

    Swagger UI â†’ http://localhost:8080/docs


Interactive, test endpoints, upload CSVs, copy curl examples.

ReDoc UI â†’ http://localhost:8080/redoc


Static, scrollable API reference.

OpenAPI spec â†’ http://localhost:8080/openapi.json

    Swagger curl examples are Linux-style. See â€œSwagger on Windowsâ€ below.

ğŸ”® Training the model

Run locally:

conda activate phdata
python scripts\train_gbm.py

This logs metrics and artifacts to mlruns/ and saves production artifacts to model/.
Artifacts include model.pkl, model_features.json, and demographics CSV.
All logged in MLflow at http://127.0.0.1:5001

.
ğŸ“¦ Batch Scoring
From CSV

Run batch prediction on unseen examples:

curl -o data\future_unseen_examples_scores.csv ^
  -F "file=@data\future_unseen_examples.csv" ^
  "http://localhost:8080/v1/predict_batch?output=csv"

The output is saved to data/future_unseen_examples_scores.csv.
From JSON (on-demand scoring)

Double-click scripts/run_json_tiny.bat or run:

scripts\run_json_tiny.bat

This:

    Creates two JSON payloads

    Calls /v1/predict

    Saves results to data/future_unseen_examples_scores_tiny.json

âš¡ Swagger on Windows

Swagger (/docs) shows curl commands in Linux format. On Windows:

    Use ^ for line continuation, not \

    Use " double quotes, not '

    Ensure file paths are correct

Example (relative path from repo root):

curl -X POST "http://localhost:8080/v1/predict_batch?output=json" ^
  -H "accept: application/json" ^
  -H "Content-Type: multipart/form-data" ^
  -F "file=@data\future_unseen_examples.csv;type=text/csv"

Save results cleanly

By default, responses dump inline (messy). Use -o:

curl -o data\future_unseen_examples_scores.json ^
  -X POST "http://localhost:8080/v1/predict_batch?output=json" ^
  -H "accept: application/json" ^
  -H "Content-Type: multipart/form-data" ^
  -F "file=@data\future_unseen_examples.csv;type=text/csv"

ğŸ–¼ï¸ Pretty Printing JSON

The saved JSON is compact (one line). To generate a human-friendly copy:

:: Convert to pretty JSON
powershell -Command "Get-Content data\future_unseen_examples_scores.json | ConvertFrom-Json | ConvertTo-Json -Depth 5 | Set-Content data\future_unseen_examples_scores_pretty.json"

Now youâ€™ll have both:

    data/future_unseen_examples_scores.json â†’ raw compact (pipeline-friendly)

    data/future_unseen_examples_scores_pretty.json â†’ indented, human-readable

ğŸ¬ One-Click Demo Scripts

For convenience, a few .bat scripts are included under scripts/ so you can test everything without typing long curl commands.
1. Batch scoring (CSV)

scripts\run_batch_csv.bat

    Calls /v1/predict_batch?output=csv

    Saves results to: data/future_unseen_examples_scores.csv

2. Batch scoring (JSON with pretty-print)

scripts\run_batch_pretty.bat

    Calls /v1/predict_batch?output=json

    Saves compact JSON â†’ data/future_unseen_examples_scores.json

    Saves pretty JSON â†’ data/future_unseen_examples_scores_pretty.json

3. On-demand (tiny JSON payloads)

scripts\run_json_tiny.bat

    Sends two handcrafted JSON records to /v1/predict

    Shows execution time & prediction results

    Saves to: data/future_unseen_examples_scores_tiny.json

âš™ï¸ Environments: Serving vs Training

Serving (Docker)

    The API container is self-contained.

    Uses the committed baseline model (model/model.pkl) so anyone can run the API immediately.

    Start services with:

    docker compose up --build

Training (Host Conda)

    Training is done outside Docker with a Conda environment.

    The spec is provided in environment.yml.

    Create the environment:

conda env create -f environment.yml -n phdata
conda activate phdata
python scripts\train_gbm.py

This regenerates model/model.pkl and logs a new run to MLflow at http://127.0.0.1:5001

.

After retraining, restart API to pick up the new model:

    docker compose restart api

âœ… Demo checklist

    Train â†’ python scripts/train_gbm.py

    Start â†’ docker compose up --build

    Health check â†’ curl http://localhost:8080/healthz

    Batch predict â†’ CSV & JSON demos

    MLflow UI â†’ confirm metrics + artifacts at http://127.0.0.1:5001

    API docs â†’ show /docs and /redoc

ğŸ“‚ Repo Structure

phdata-mle-solution/
â”‚
â”œâ”€â”€ app/                  # FastAPI app
â”‚   â””â”€â”€ main.py           # API routes
â”œâ”€â”€ scripts/              # Training + demo scripts
â”‚   â”œâ”€â”€ train_gbm.py
â”‚   â”œâ”€â”€ run_batch_demo.bat
â”‚   â”œâ”€â”€ run_batch_csv.bat
â”‚   â”œâ”€â”€ run_batch_pretty.bat
â”‚   â””â”€â”€ run_json_tiny.bat
â”œâ”€â”€ data/                 # Input + outputs
â”‚   â””â”€â”€ future_unseen_examples.csv
â”œâ”€â”€ model/                # Exported artifacts
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md

ğŸ¯ Notes

    Swagger UI â†’ great for reviewers to test interactively

    ReDoc â†’ nice static reference

    MLflow â†’ full experiment history & artifacts

    Curl â†’ cross-platform, but Windows tweaks are documented

    Predictions â†’ available via both CSV batch and JSON on-demand